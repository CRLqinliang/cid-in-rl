import cid.algorithms
import cid.influence_estimation
import cid.envs
import cid.envs.robotics
import cid.memory
import cid.models
import cid.models.layers
import cid.rollouts
import cid.utils.gin_torch_externals

main.experiment_group = 'FetchPickAndPlace/exploration/cai_bonus_0.2'
main.experiment_name = 'cai_bonus_0.2'
main.env_fn = @envs.make_gym_env
main.agent_fn = @ddpg/algorithms.make_agent
main.warmup_agent_fn = @warmup/algorithms.make_agent
main.rollout_gen_cls = @GoalBasedRolloutGenerator
main.replay_memory_cls = @MBPReplayMemory
main.model_setup_fn = @make_scorer_and_model_trainer
main.episode_len = 50
main.seed = 1

envs.make_gym_env.name = 'FetchPickAndPlace-v1'
envs.get_env_reward_fn.env_fn = @envs.make_gym_env

train.n_iterations = 20000
train.n_updates_per_step = 20
train.batch_size = 256
train.buffer_warmup = 200

train.log_every = 200
train.evaluate_every = 200
train.save_every = 20000
train.train_model_schedule = @batch_train_schedule

test.n_episodes = 100

MBPReplayMemory.size = 10000
MBPReplayMemory.reward_fn = @envs.get_env_reward_fn()
MBPReplayMemory.p_replay = 0.8
MBPReplayMemory.prioritize_episodes = False
MBPReplayMemory.prioritize_transitions = False
MBPReplayMemory.bonus_type = "additive"
MBPReplayMemory.influence_bonus = 0.2
MBPReplayMemory.bonus_score_max = 2.0
MBPReplayMemory.reward_max = 0

warmup/algorithms.make_agent.agent_cls = @random_agent.RandomAgent
warmup/algorithms.make_agent.goal_based = True

ddpg/algorithms.make_agent.agent_cls = @ddpg_agent.DDPGAgent
ddpg/algorithms.make_agent.goal_based = True

DDPGAgent.pi_cls = @ddpg/MLP
DDPGAgent.q_cls = @ddpg/MLP
DDPGAgent.gamma = 0.98
DDPGAgent.polyak = 0.95
DDPGAgent.action_l2_penalty = 1.0
DDPGAgent.action_noise = 0.2
DDPGAgent.random_eps = 0.2
DDPGAgent.observation_clip = (-5, 5)
DDPGAgent.q_target_clip = (-50, 0)
DDPGAgent.normalize_state = True
DDPGAgent.normalizer_warmup_updates = 200

ddpg/MLP.hidden_dims = [256, 256, 256]
ddpg/MLP.weight_init = @init.xavier_uniform_
ddpg/MLP.bias_init = @init.zeros_

make_scorer_and_model_trainer.scorer_cls = @CMIScorer
make_scorer_and_model_trainer.model_classes = {
    'full_model': @full_model/MLP
}
make_scorer_and_model_trainer.dataset_classes = {
    'full_model': @full_model/ForwardGoalDataset
}

ForwardGoalDataset.target_scale = 50
ForwardGoalDataset.copy_memory = True

layers.GaussianLikelihoodHead.min_var = 1e-8
layers.GaussianLikelihoodHead.max_var = 200
layers.GaussianLikelihoodHead.use_spectral_norm_var = True

full_model/MLP.hidden_dims = [256, 256, 256, 256]
full_model/MLP.weight_init = @init.orthogonal_
full_model/MLP.bias_init = @init.zeros_
full_model/MLP.hidden_activation = @activation.Tanh
full_model/MLP.bn_first = True
full_model/MLP.outp_layer = @layers.GaussianLikelihoodHead
full_model/MLP.use_spectral_norm = False

ModelTrainer.batch_size = 500
ModelTrainer.loss_fn = @gaussian_log_likelihood_loss
ModelTrainer.lr = 8e-4
ModelTrainer.optimizer_kwargs = {'betas': (0.9, 0.9)}
OnlineModelTrainer.batch_mode = True
OnlineModelTrainer.warmup_epochs = None
OnlineModelTrainer.epochs_per_step = None
OnlineModelTrainer.full_rescore_every = 1
OnlineModelTrainer.rescore_n_latest_episodes = 1000
batch_train_schedule.train_model_every = 100

CMIScorer.n_expectation_samples = 32
CMIScorer.n_mixture_samples = 32
CMIScorer.reuse_action_samples = True
