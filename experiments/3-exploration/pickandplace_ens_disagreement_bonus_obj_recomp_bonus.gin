import cid.algorithms
import cid.influence_estimation
import cid.envs
import cid.memory
import cid.models
import cid.rollouts
import cid.utils.gin_torch_externals

main.experiment_group = 'FetchPickAndPlace/exploration/ens_disagreement_bonus_obj_recomp_bonus'
main.experiment_name = 'ens_disagreement'
main.env_fn = @envs.make_gym_env
main.agent_fn = @ddpg/algorithms.make_agent
main.warmup_agent_fn = @warmup/algorithms.make_agent
main.rollout_gen_cls = @GoalBasedRolloutGenerator
main.replay_memory_cls = @MBPReplayMemory
main.model_setup_fn = @make_scorer_and_model_trainer
main.episode_len = 50
main.seed = 1

envs.make_gym_env.name = 'FetchPickAndPlace-v1'
envs.get_env_reward_fn.env_fn = @envs.make_gym_env

train.n_iterations = 20000
train.n_updates_per_step = 20
train.batch_size = 256
train.buffer_warmup = 200

train.train_model_every = 100
train.log_every = 200
train.evaluate_every = 200
train.save_every = 10000
train.train_model_schedule = @batch_train_schedule

test.n_episodes = 100

MBPReplayMemory.size = 10000
MBPReplayMemory.reward_fn = @envs.get_env_reward_fn()
MBPReplayMemory.p_replay = 0.8
MBPReplayMemory.prioritize_episodes = False
MBPReplayMemory.prioritize_transitions = False
MBPReplayMemory.bonus_type = "additive"
MBPReplayMemory.influence_bonus = 50
MBPReplayMemory.bonus_score_max = 0.1
MBPReplayMemory.reward_max = 0

warmup/algorithms.make_agent.agent_cls = @random_agent.RandomAgent
warmup/algorithms.make_agent.goal_based = True

ddpg/algorithms.make_agent.agent_cls = @ddpg_agent.DDPGAgent
ddpg/algorithms.make_agent.goal_based = True

DDPGAgent.pi_cls = @ddpg/MLP
DDPGAgent.q_cls = @ddpg/MLP
DDPGAgent.gamma = 0.98
DDPGAgent.polyak = 0.95
DDPGAgent.action_l2_penalty = 1.0
DDPGAgent.action_noise = 0.2
DDPGAgent.random_eps = 0.3
DDPGAgent.observation_clip = (-5, 5)
DDPGAgent.q_target_clip = (-50, 0)
DDPGAgent.normalize_state = True
DDPGAgent.normalizer_warmup_updates = 200

ddpg/MLP.hidden_dims = [256, 256, 256]
ddpg/MLP.weight_init = @init.xavier_uniform_
MLP.bias_init = @init.zeros_

make_scorer_and_model_trainer.scorer_cls = @EnsembleDisagreementScorer
make_scorer_and_model_trainer.model_trainer_cls = @EnsembleTrainer
make_scorer_and_model_trainer.model_classes = {
    'model': @Ensemble
}
make_scorer_and_model_trainer.dataset_classes = {
    'model': @full_model/ForwardGoalDataset
}

ForwardGoalDataset.target_scale = 50
ForwardGoalDataset.copy_memory = True

Ensemble.model_cls = @full_model/MLP
Ensemble.n_models = 5

full_model/MLP.hidden_dims = [256, 256, 256, 256]
full_model/MLP.weight_init = @init.orthogonal_
full_model/MLP.bias_init = @init.zeros_
full_model/MLP.hidden_activation = @activation.Tanh
full_model/MLP.bn_first = True
full_model/MLP.legacy_bn_first = False
full_model/MLP.use_spectral_norm = False

ModelTrainer.batch_size = 500
ModelTrainer.loss_fn = @mse_loss
ModelTrainer.lr = 8e-4
ModelTrainer.optimizer_kwargs = {'betas': (0.9, 0.9)}
OnlineModelTrainer.batch_mode = True
OnlineModelTrainer.epochs_per_step = None
OnlineModelTrainer.warmup_epochs = None
OnlineModelTrainer.n_dataloader_workers = 0
OnlineModelTrainer.full_rescore_every = 1
OnlineModelTrainer.rescore_n_latest_episodes = 1000
batch_train_schedule.train_model_every = 100
batch_train_schedule.multiplier_initial = 1
batch_train_schedule.multiplier_start = 1
batch_train_schedule.multiplier_interm = 1
batch_train_schedule.stop_after = 20000